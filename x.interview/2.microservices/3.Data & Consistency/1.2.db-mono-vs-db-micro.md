# üîÑ Read/Write Splits & Replicas ‚Äî ‚ÄúWhole app DB‚Äù vs **Microservices**

**Short answer:** The ‚Äúone write DB + many read replicas for the whole app‚Äù is a **monolithic database pattern**. In **microservices**, you still can (and often should) use **read/write splitting**, but you do it **per service‚Äôs own database**, not one shared cluster for everyone.

---

## üß≠ Why a single global primary+replicas is **not** microservices

| Problem if all services share one DB/replicas | Why it hurts                                                                              |
| --------------------------------------------- | ----------------------------------------------------------------------------------------- |
| **Coupling & coordination**                   | A schema change for Inventory risks breaking Orders/Payments; independent deploys vanish. |
| **Blast radius**                              | Primary outage = **entire platform** down.                                                |
| **Performance interference**                  | One service‚Äôs heavy reads consume replica bandwidth, starving others.                     |
| **Hidden joins**                              | Teams ‚Äújust join a table‚Äù ‚Üí you‚Äôve rebuilt a distributed monolith.                        |
| **Scaling mismatch**                          | Catalog read traffic ‚â´ Payments write traffic‚Äîbut everyone scales the same DB tier.       |

**Microservices rule:** **DB-per-service.** If you want read replicas, you attach them **to that service‚Äôs DB only**.

---

## ‚úÖ Microservices-friendly pattern

```mermaid
flowchart LR
  subgraph Ordering Service
    OAPI[Ordering API pods] -->|writes/critical reads| ODBP[(Orders Primary)]
    OAPI -->|read-mostly| ODBR[(Orders Read Replicas)]
  end

  subgraph Inventory Service
    IAPI[Inventory API pods] -->|writes/critical reads| IDBP[(Inventory Primary)]
    IAPI -->|read-mostly| IDBR[(Inventory Read Replicas)]
  end

  OAPI -. emits events .-> BUS[(Broker)]
  IAPI -. emits events .-> BUS
  BUS --> Proj[(Read Models / Projections per need)]
```

- Each service owns **its** primary + optional replicas.
- Cross-service data flows via **events** and **read models**, not joins.

---

## üîó ‚ÄúRelations‚Äù & joins in this world

- **Inside one service** (same DB): use normal **FKs, joins, transactions**.
- **Across services**: store **foreign IDs** (no FK), and either:

  - call the other service **synchronously** for validation (sparingly), or
  - keep a **projection** updated by events (preferred for read paths).

- For stable views (e.g., price at purchase), **snapshot** fields into your own tables at write time.

---

## üìñ Read replicas with **eventual consistency** ‚Äî how to live with it

Read replicas lag behind the primary by milliseconds‚Üíseconds. Tactics:

1. **Read-your-write** when needed: route those requests to the **primary**.
2. **Session pinning**: after a write, mark the user/session to use primary for N seconds.
3. **LSN/Change version check** (advanced): include the commit version (LSN/timestamp) in the response; the client sends it back and the API waits/reads from primary until replicas ‚â• version.
4. **Fallback-on-miss**: if a fresh read can‚Äôt find the expected row on a replica, retry on **primary**.
5. **Cache & invalidate**: for hot keys, cache after write and serve until replicas catch up.

> Rule of thumb: **critical, immediate reads ‚Üí primary**; **general reads ‚Üí replicas**.

---

## üßë‚Äçüíª .NET ways to implement read/write splitting

### Option A ‚Äî Two DbContexts (clear separation)

```csharp
builder.Services.AddDbContext<OrdersWriteDb>(o =>
    o.UseSqlServer(cfg.WriteConn, x => x.EnableRetryOnFailure()));

builder.Services.AddDbContextFactory<OrdersReadDb>(o =>
    o.UseSqlServer(cfg.ReadConn, x => x.EnableRetryOnFailure())
     .UseQueryTrackingBehavior(QueryTrackingBehavior.NoTracking));
```

```csharp
public class OrdersQueryService {
  private readonly IDbContextFactory<OrdersReadDb> _factory;
  public OrdersQueryService(IDbContextFactory<OrdersReadDb> factory) => _factory = factory;

  public async Task<OrderDto?> GetAsync(Guid id) {
    await using var db = await _factory.CreateDbContextAsync();
    return await db.Orders.AsNoTracking().Where(o => o.Id == id).Select(...).FirstOrDefaultAsync();
  }
}
```

### Option B ‚Äî One context, connection routing

- Use a repository/Unit-of-Work that opens a **read** or **write** connection per operation.
- For Azure SQL: set `ApplicationIntent=ReadOnly` on the **read** connection string to hit readable secondaries.

```ini
# appsettings.json (examples)
"ConnectionStrings": {
  "OrdersWrite": "Server=...;Database=Orders;Encrypt=true;",
  "OrdersRead":  "Server=...;Database=Orders;Application Intent=ReadOnly;Encrypt=true;"
}
```

**Don‚Äôt forget:**

- Transient-fault handling (`EnableRetryOnFailure()`).
- Concurrency tokens (`rowversion`) to prevent lost updates.
- Query behavior `NoTracking` for reads (less overhead).

---

## üß™ Where CQRS fits

- **CQRS inside a service**: separate **write model** (aggregate invariants) from **read model** (denormalized, replica-friendly).
- This is orthogonal to ‚Äúreplicas‚Äù; you can have CQRS on a single primary or on primary+replicas.

---

## üåç Multi-region notes (in case it comes up)

- Most relational setups are **single-writer** + geo-replicated **readers** ‚Üí same rules: pin critical reads to the region‚Äôs writer.
- True multi-master systems exist (e.g., some NewSQL/NoSQL), but they trade off latency/conflict handling‚Äîrarely needed for typical microservice CRUD.

---

## ‚ùå When is a **shared** replica farm OK?

- For **analytics/BI** only: stream CDC/events into a **central warehouse** (Synapse/BigQuery/Databricks) and do big joins **there**.
- **Never** use a shared OLTP replica cluster as the runtime read DB for multiple services‚Äîit re-couples them.

---

## üìù Crisp answers for the assessment

- **Is ‚Äú1 write + many reads‚Äù the same in microservices?**
  **No** if it‚Äôs **one cluster for the whole app** (anti-pattern).
  **Yes** if it‚Äôs **per service** (good pattern).

- **How do multiple pods see consistent data?**
  They share the **same primary**; replicas are for **read-mostly**, with **read-your-write ‚Üí primary** and **optimistic concurrency** to guard writes.

- **How do you do cross-service ‚Äújoins‚Äù?**
  You **don‚Äôt**. Use **IDs**, **events**, and **projections/read models**.

- **How do you prevent duplicate effects with retries?**
  **Idempotency keys** + **unique constraints** + **Inbox/Outbox** patterns.

---

## ‚úÖ Pocket checklist

- [ ] DB-per-service (primary + optional replicas)
- [ ] Intra-service joins allowed; inter-service joins forbidden
- [ ] Read-your-write routes to **primary**; general reads to **replicas**
- [ ] `rowversion` + retries (optimistic concurrency)
- [ ] Outbox/Inbox + idempotency; DLQ & replay
- [ ] Analytics/reporting done **outside** OLTP, not by sharing replicas

---

> **One-liner to remember:** _Use read replicas as an **internal optimization per service**, not a shared crutch across services. Join inside, publish facts outside._
