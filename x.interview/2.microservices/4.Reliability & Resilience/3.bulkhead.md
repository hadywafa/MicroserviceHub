# 🧱 Bulkheads & Isolation — limit the blast radius (simple, practical)

> Picture a ship with watertight **bulkheads**. If one compartment floods, the ship still floats.
> In microservices, bulkheads mean: **put hard limits and separate “compartments”** so one slow/broken dependency can’t sink everything.

---

## 🧠 Plain-English idea

- **Bulkhead = a cap**: “Only **N** calls to Payments at once. After that, **reject fast** instead of piling up.”
- **Isolation = separate lanes**: “Payments calls use their **own lane** (connections/threads/queue) so Catalog issues can’t block them.”

Use both together to **contain failure, keep latency predictable**, and **save the rest of the app**.

---

## 🧩 Where to apply bulkheads (quick map)

1. **Per dependency HTTP/gRPC client** — its own connection pool + concurrency cap
2. **Per message consumer** — limit concurrent handlers, prefetch size
3. **Per database** — max connection pool per service
4. **Per work type** — separate background workers/queues (e.g., emails vs invoices)
5. **At platform level** — K8s CPU/memory limits, HPA/KEDA, separate pods

---

## ✅ Simple rules you can remember

- **Cap concurrency** per downstream (`max 50` in parallel), **0 or tiny queue** (don’t pile up requests).
- **Separate pools**: different `HttpClient`s for Catalog vs Payments, different DB pools, different message processors.
- **Fail fast** (HTTP 429/503) when a lane is full; **degrade** (cached/partial) instead of timing out.
- Combine with **timeouts, retries, circuit breakers**.

---

## 👨‍💻 .NET quick snippets

### 1) HTTP/gRPC bulkhead + connection limits (per dependency)

```csharp
using Polly;

var bulkhead = Policy.BulkheadAsync<HttpResponseMessage>(
    maxParallelization: 50,          // at most 50 in-flight calls
    maxQueuingActions: 0,            // don't queue; reject fast
    onBulkheadRejectedAsync: _ => Task.CompletedTask);

builder.Services.AddHttpClient("payments", c =>
{
    c.BaseAddress = new Uri(cfg.PaymentsUrl);
})
.ConfigurePrimaryHttpMessageHandler(() => new SocketsHttpHandler
{
    MaxConnectionsPerServer = 50,    // connection pool cap (same idea)
    PooledConnectionLifetime = TimeSpan.FromMinutes(5)
})
.AddPolicyHandler(bulkhead);
// (also add timeouts, retries, breaker as in previous topics)
```

> Why no queue? Queues hide overload and **increase latency**. If you must queue, keep it tiny.

### 2) Database connection pool (per service)

```ini
# connection string
Server=...;Database=Orders;Max Pool Size=100;Encrypt=true;
```

- Keep pool sizes realistic to avoid DB exhaustion. If you scale pods, ensure the **sum** of pools won’t overwhelm DB.

### 3) Azure Service Bus / RabbitMQ consumer concurrency

```csharp
// Azure Service Bus
var processor = client.CreateProcessor("orders", new ServiceBusProcessorOptions {
  MaxConcurrentCalls = 8,          // bulkhead for handlers
  PrefetchCount = 32,
  AutoCompleteMessages = false
});
```

```csharp
// RabbitMQ (prefetch = bulkhead)
channel.BasicQos(prefetchSize: 0, prefetchCount: 32, global: false);
```

### 4) Separate lanes for “heavy” work

- Put **PDF generation** or **report exports** on their **own queue** and worker deployment (own CPU/memory limits & autoscaling). If they spike, they don’t block checkout.

---

## ☸️ Kubernetes-side isolation (one-liners)

- **Requests/Limits**: give each deployment CPU/memory **requests/limits** to avoid noisy neighbors.
- **HPA/KEDA**: autoscale **per service** (or per queue length) independently.
- **PodDisruptionBudget**: keep enough replicas of critical paths.
- Optional: **node affinity** to keep critical workloads on roomier nodes.

---

## 🧪 How to respond when a bulkhead is full

- **Return 429/503** quickly (with `Retry-After`) instead of waiting 5s.
- **Fallback** to cached/partial data if safe.
- **Queue for later** only if the business allows asynchronous processing.

---

## 🚨 Common mistakes → quick fixes

| Mistake                       | Why it hurts                     | Fix                                             |
| ----------------------------- | -------------------------------- | ----------------------------------------------- |
| One `HttpClient` for all deps | One slow dep ties up all sockets | **One client per dependency** with its own caps |
| Large in-memory queues        | Latency spikes, OOM under load   | **Tiny/no queue**, fail fast                    |
| Unlimited DB connections      | DB thrash, timeouts              | **Max Pool Size**, watch totals across pods     |
| One worker for all jobs       | Heavy tasks starve critical ones | **Separate queues/workers**                     |
| Only retries/breakers         | Still overload during spikes     | Add **bulkheads** to cap concurrency            |

---

## 🗣️ Interview sound bites

- “**Bulkheads** cap concurrency per dependency; **isolation** gives each dependency its own lane (connections, threads, queues).”
- “If a dependency misbehaves, we **fail fast** on that lane so other features still work.”
- “We combine bulkheads with **timeouts, retries, and circuit breakers** for full resilience.”

---

## ✅ Pocket checklist

- [ ] Per-dependency `HttpClient` with **MaxConnectionsPerServer**
- [ ] **Polly Bulkhead** (cap concurrency, no or tiny queue)
- [ ] DB **Max Pool Size** tuned; know total across pods
- [ ] Separate queues/workers for heavy vs critical work
- [ ] K8s **requests/limits**, HPA/KEDA per deployment
- [ ] Fast fallback (429/503/cached) when bulkhead is full

> **One-liner:** _Give each dependency its own lane with a hard speed limit—if that lane clogs, the rest of the highway keeps moving._
