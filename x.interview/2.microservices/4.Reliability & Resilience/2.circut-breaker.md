# 🧯 Circuit Breaker — stop cascading failures (super simple)

> **Purpose:** When a downstream service is **sick** (slow/errors), stop hammering it. **Fail fast** instead of waiting & retrying forever. Protect your app and let the downstream recover.

---

## 🧠 What it does (human version)

- Think of a **fuse** in electricity:

  - **Closed** (normal): traffic flows.
  - Too many recent failures? **Open**: **instantly fail** new calls (don’t even try).
  - After a short **cool-down**, **Half-open**: let a **few test calls** through.

    - If tests succeed → **Close** (back to normal).
    - If tests fail → back to **Open** (keep protecting).

This prevents:

- **Cascading failures** (one slow service freezes everything)
- **Retry storms** (thousands of callers retrying a dying service)

---

## 🧩 How it fits with timeouts & retries

- **Timeouts**: “Don’t wait too long.”
- **Retries**: “Try again a couple times (if transient).”
- **Circuit breaker**: “We’ve seen enough failures—**stop trying for a bit**.”
- **Bulkhead** (next topic): “Only N concurrent calls to that dependency.”

Use all four together.

---

## 🎛️ Sensible defaults (numbers you can say)

- Trip after **5** consecutive failures (or high failure rate over the last \~10–20 calls).
- Stay open (cool-down) for **30 seconds**.
- Half-open: allow **1–5** trial requests.
- Pair with per-try **timeouts** (e.g., 250 ms) and **1–2** retries max.

Tune per dependency and monitor.

---

## 🧑‍💻 .NET with Polly (tiny snippet)

```csharp
using Polly;
using Polly.CircuitBreaker;

var breaker = Policy<HttpResponseMessage>
  .Handle<HttpRequestException>()                       // network faults
  .OrResult(r => (int)r.StatusCode >= 500)              // 5xx from server
  .CircuitBreakerAsync(
      handledEventsAllowedBeforeBreaking: 5,            // trip threshold
      durationOfBreak: TimeSpan.FromSeconds(30));       // cool-down

builder.Services.AddHttpClient("catalog", c =>
{
    c.BaseAddress = new Uri(cfg.CatalogUrl);
})
.AddPolicyHandler(breaker);                              // add timeouts/retries too
```

**Tip:** Log breaker **state changes** and **correlation IDs** so you can see when/why it opened.

---

## 🧰 What to return while “open”

- **Fast error** (e.g., 503 “Service Unavailable”)
- **Fallback** (cached/last-known data, degraded info)
- **Queue the command** for later (if appropriate) and tell the user it’s processing

Pick one that matches your business.

---

## 🚨 Common mistakes → quick fixes

| Mistake                                            | Fix                                                    |
| -------------------------------------------------- | ------------------------------------------------------ |
| Only retries, no breaker → stampede during outages | Add a **circuit breaker** with cool-down               |
| Breaker but no timeouts → calls still hang         | Set **tight timeouts** per try                         |
| One breaker shared for all deps                    | Use **per-dependency** breakers                        |
| Open forever (no recovery)                         | Use **half-open** test window                          |
| No telemetry                                       | Track **open/half-open counts**, failure rate, latency |

---

## 📝 Interview sound bites

- “A circuit breaker **fails fast** after repeated errors to prevent cascades, then **probes** recovery with a half-open state.”
- “It **works with** timeouts (bound latency) and retries (transient hiccups). When failure persists, the breaker **opens**.”
- “We configure breakers **per dependency**, log state changes, and provide **fallbacks** where possible.”

> **One-liner:** _When a dependency is clearly sick, stop calling it for a while—fail fast, recover safely._
